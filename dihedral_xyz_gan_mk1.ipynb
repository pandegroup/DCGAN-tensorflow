{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, rdMolTransforms, rdmolops\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.mixture import GMM\n",
    "import random\n",
    "from tflearn.activations import leaky_relu\n",
    "from deepchem.utils.rdkit_util import get_xyz_from_mol\n",
    "from deepchem.feat.rdkit_grid_featurizer import convert_atom_to_voxel, compute_centroid, rotate_molecules\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.feat.graph_features import ConvMolFeaturizer\n",
    "from deepchem.feat.adjacency_fingerprints import AdjacencyFingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_mols(mol_files, max_n_atoms):\n",
    "    featurizer = AdjacencyFingerprint(max_n_atoms=max_n_atoms, n_bonds=5)\n",
    "    features = []\n",
    "    for idx, mol_file in enumerate(mol_files):\n",
    "        if idx % 32 == 0:\n",
    "            print(idx)\n",
    "        try:\n",
    "            if \".pdb\" in mol_file:\n",
    "                mol = Chem.MolFromPDBFile(mol_file)\n",
    "            else:\n",
    "                mol = Chem.MolFromMol2File(mol_file)\n",
    "                \n",
    "            mol_xyz = get_xyz_from_mol(mol)\n",
    "            centroid = compute_centroid(mol_xyz)\n",
    "            mol_xyz -= centroid\n",
    "            temp = np.zeros((max_n_atoms, 3))\n",
    "            temp[:mol_xyz.shape[0]] = mol_xyz\n",
    "            mol_xyz = temp\n",
    "            \n",
    "            torsions = []\n",
    "            torsion_tuples = []\n",
    "\n",
    "            torsion_matrix = np.zeros((n_bonds,1))\n",
    "            torsion_indices = np.zeros((n_bonds,max_n_atoms,4)).astype(np.uint8)\n",
    "            subgraphs = np.zeros((n_bonds, max_n_atoms, max_n_atoms)).astype(np.uint8)\n",
    "            \n",
    "            #mol_xyz = rotate_molecules([mol_xyz])[0]\n",
    "\n",
    "            c = mol.GetConformer(0)\n",
    "            idx = 0\n",
    "            for bond in mol.GetBonds():\n",
    "                if bond.IsInRing(): \n",
    "                    continue\n",
    "                def calc_torsions(atom_i, atom_j, bond_idx):        \n",
    "                    exist_dihed = False\n",
    "                    for neighbor_j in atom_j.GetNeighbors():\n",
    "                        if neighbor_j.GetIdx() == atom_i.GetIdx():\n",
    "                            continue\n",
    "\n",
    "                        dihed_idx = 0\n",
    "                        for neighbor_i in atom_i.GetNeighbors():\n",
    "                            if neighbor_i.GetIdx() == atom_j.GetIdx():\n",
    "                                continue\n",
    "\n",
    "                            exist_dihed=True\n",
    "                            torsion_tuple = (neighbor_i.GetIdx(), atom_i.GetIdx(), atom_j.GetIdx(), neighbor_j.GetIdx())\n",
    "\n",
    "                            if get_angle:\n",
    "                                torsion_matrix[bond_idx][dihed_idx] = rdMolTransforms.GetDihedralRad(c, *torsion_tuple)\n",
    "                            torsion_indices[bond_idx][torsion_tuple[0]][dihed_idx*4] = 1\n",
    "                            torsion_indices[bond_idx][torsion_tuple[1]][dihed_idx*4+1] = 1\n",
    "                            torsion_indices[bond_idx][torsion_tuple[2]][dihed_idx*4+2] = 1\n",
    "                            torsion_indices[bond_idx][torsion_tuple[3]][dihed_idx*4+3] = 1\n",
    "                            \n",
    "                            broken_mol = rdmolops.FragmentOnBonds(mol, [bond.GetIdx()], addDummies=False)\n",
    "                            molfrags = GetMolFrags(broken_mol)\n",
    "                            if bond.GetEndAtom().GetIdx() in molfrags[0]:\n",
    "                                atoms_to_rotate = molfrags[0]\n",
    "                            else:\n",
    "                                atoms_to_rotate = molfrags[1]\n",
    "\n",
    "                            subgraph = np.zeros((max_n_atoms, max_n_atoms)).astype(np.uint8)\n",
    "                            for atom_idx in atoms_to_rotate:\n",
    "                                subgraph[atom_idx][atom_idx] = 1\n",
    "                            subgraphs[bond_idx, :, :] = subgraph\n",
    "                \n",
    "                            bond_idx += 1\n",
    "                        break\n",
    "                    return(bond_idx)\n",
    "                \n",
    "                idx = calc_torsions(bond.GetBeginAtom(), bond.GetEndAtom(), idx)\n",
    "                \n",
    "            graph_feat = featurizer.featurize([mol])[0]           \n",
    "            features.append((mol_file, mol_xyz, torsion_indices, torsion_matrix, subgraphs, graph_feat))\n",
    "        except:\n",
    "            features.append(None)\n",
    "    return(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "max_n_atoms = 12\n",
    "S = 64\n",
    "feature_file = \"./voxel_features_pdbbind.pkl\"\n",
    "if not os.path.exists(feature_file):\n",
    "#if 1== 1:\n",
    "    pdbbind_dir = \"/home/evan/Documents/deep_docking/datasets/v2015/\"\n",
    "    def find_files(directory, pattern):\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for basename in files:\n",
    "                if fnmatch.fnmatch(basename, pattern):\n",
    "                    filename = os.path.join(root, basename)\n",
    "                    yield filename\n",
    "    ligand_files = []\n",
    "    for f in find_files(pdbbind_dir, \"*ligand.mol2\"):\n",
    "        ligand_files += [f]\n",
    "    ligand_files = ligand_files[:] + [\"/home/evan/Documents/deep_docking/alanine_dipeptide.pdb\"]*S  \n",
    "    features = featurize_mols(ligand_files, max_n_atoms)\n",
    "    with open(feature_file, \"wb\") as f:\n",
    "        pickle.dump(features, f, protocol=2)\n",
    "else:\n",
    "    with open(feature_file, \"rb\") as f:\n",
    "        features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [f for f in features if f is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "alpha = 0.01\n",
    "n_layers = 1\n",
    "\n",
    "S = 32\n",
    "\n",
    "B = max_n_atoms\n",
    "p = 75\n",
    "\n",
    "z_dim = 64\n",
    "\n",
    "L_list = [p, 64, 64, 128, 256]\n",
    "\n",
    "dihed_per_bond = 1\n",
    "valence = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    x = tf.placeholder(tf.float32, [S, B, L_list[0]], name=\"atom_features\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    adj_matrix = tf.placeholder(tf.float32, shape=[S, B, B], name=\"adj_matrix\")\n",
    "    \n",
    "    non_adj_matrix = tf.subtract(tf.ones_like(adj_matrix), adj_matrix)\n",
    "    \n",
    "    #dist_matrix2d = tf.placeholder(tf.float32, shape=[S, B, B], name=\"dist_matrix2d\")\n",
    "    #bond_dist_mat = tf.placeholder(tf.float32, shape=[S, B, B], name=\"bond_dist_mat\")\n",
    "    \n",
    "    #angle_tensor = tf.placeholder(tf.float32, shape=[S, B, B, B], name=\"angle_tensor\")\n",
    "    angle_tuples = tf.placeholder(tf.float32, shape=[S, B, B, B], name=\"angle_tuples\")\n",
    "    n_bonds = tf.placeholder(tf.float32, shape=[S,1], name=\"nbonds\")\n",
    "    n_angles = tf.placeholder(tf.float32, shape=[S,1], name=\"nangles\")\n",
    "    \n",
    "    \n",
    "    mol_xyz = tf.placeholder(tf.float32, shape=[S, B, 3], name=\"molxyz\")\n",
    "    \n",
    "    z = tf.random_normal([S, B, z_dim], mean=0, stddev=1)\n",
    "    \n",
    "    mol_noise = tf.random_normal([S,B,3], mean=0,stddev=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adapted from: http://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow\n",
    "def compute_distance_matrix(A):\n",
    "    r = tf.reduce_sum(A*A, 1) # turn r into column vector \n",
    "    r = tf.reshape(r, [-1, 1]) \n",
    "    D = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r)\n",
    "    return(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GW_list = [None for i in range(n_layers)]\n",
    "Gb_list = [None for i in range(n_layers)]\n",
    "\n",
    "DW_list = [None for i in range(n_layers)]\n",
    "Db_list = [None for i in range(n_layers)]\n",
    "\n",
    "\n",
    "Gh_list = [x]\n",
    "Dh_list = [x]\n",
    "\n",
    "adj_list = [adj_matrix]\n",
    "B_list = [B]\n",
    "clades_list = []\n",
    "graph_stride = 1.\n",
    "\n",
    "print(\"building clades and adj\")\n",
    "for i in range(n_layers):\n",
    "    print(i)\n",
    "    B_list.append(int(np.ceil(B_list[i]/graph_stride)))\n",
    "    print(B_list)\n",
    "    clades_list.append(tf.stack([tf.one_hot(range(0,B_list[i],int(graph_stride)), depth=B_list[i])]*S, axis=0)) \n",
    "\n",
    "\n",
    "    adj_temp = tf.matmul(clades_list[i], adj_list[i])\n",
    "    sub_adj = tf.matmul(adj_temp, tf.transpose(adj_temp, perm=[0, 2, 1]))\n",
    "    sub_adj = tf.minimum(sub_adj, 1.)\n",
    "    adj_list.append(sub_adj)\n",
    "\n",
    "print(\"Building hidden layers\")\n",
    "for layer_idx in range(n_layers):\n",
    "    GW_list[layer_idx] = tf.Variable(tf.truncated_normal([L_list[layer_idx], L_list[layer_idx+1]], seed=2017), name=\"GW_list%d\" %layer_idx)\n",
    "    Gb_list[layer_idx] = tf.Variable(tf.ones([1, L_list[layer_idx+1]]))\n",
    "    \n",
    "    DW_list[layer_idx] = tf.Variable(tf.truncated_normal([L_list[layer_idx], L_list[layer_idx+1]], seed=2017), name=\"DW_list%d\" %layer_idx)\n",
    "    Db_list[layer_idx] = tf.Variable(tf.ones([1, L_list[layer_idx+1]]))\n",
    "\n",
    "f_w1 = tf.Variable(tf.truncated_normal([L_list[n_layers], L_list[n_layers]]))\n",
    "f_b1 = tf.Variable(tf.ones([L_list[n_layers]]))\n",
    "\n",
    "g_w1_ini = tf.Variable(tf.truncated_normal([1, L_list[n_layers]*2+z_dim, 64]))\n",
    "g_w1 = tf.tile(g_w1_ini, [S, 1, 1])\n",
    "\n",
    "g_b1 = tf.Variable(tf.ones([1, 1, 64]))\n",
    "\n",
    "g_w2_ini = tf.Variable(tf.truncated_normal([1, 64, 3]))\n",
    "g_w2 = tf.tile(g_w2_ini, [S, 1, 1])\n",
    "g_b2 = tf.Variable(tf.ones([1, 1, 3]))\n",
    "\n",
    "d_w1_ini = tf.Variable(tf.truncated_normal([1, B*(L_list[n_layers]*2+1), 32]))\n",
    "d_w1 = tf.tile(d_w1_ini, [S, 1, 1])\n",
    "d_b1 = tf.Variable(tf.ones([1, 1, 32]))\n",
    "\n",
    "d_w2_ini = tf.Variable(tf.truncated_normal([1, 32, 1]))\n",
    "d_w2 = tf.tile(d_w2_ini, [S, 1, 1])\n",
    "d_b2 = tf.Variable(tf.ones([1, 1, 1]))\n",
    "\n",
    "lam1 = tf.Variable(tf.constant(0.01))\n",
    "lam2 = tf.Variable(tf.constant(0.01))\n",
    "lam3 = tf.Variable(tf.constant(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjacency_conv_layer(x, W, b, adj, clades, L_in, L_out, layer_idx, S, B):\n",
    "    with tf.device('/gpu:0'):\n",
    "        print(\"layer_idx: %d\" %(layer_idx))\n",
    "        h = tf.matmul(adj, x, name=\"adj_mult_%d\" %layer_idx)\n",
    "        h = tf.reshape(h, shape=(S*B, L_in), name=\"adj_reshape_1_%d\" %layer_idx)\n",
    "\n",
    "        h = tf.matmul(h, W, name=\"adjconv_%d\" %layer_idx) + b\n",
    "    \n",
    "        h = tf.nn.tanh(h)\n",
    "        h = tf.reshape(h, (S, B, L_out), name=\"adj_reshape_2_%d\" %layer_idx)  \n",
    "\n",
    "        h = tf.matmul(clades, h)\n",
    "\n",
    "        print(\"within func h:\")\n",
    "        print(h)                                                                                                                              \n",
    "\n",
    "    return(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t = sess.run(tf.matmul(dihed_indices[:,:,:,0], x))\n",
    "#t = np.reshape(t, [t.shape[0]*t.shape[1],t.shape[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_angle_tensor(mols_batch, angle_tuples_batch):\n",
    "    i_vec = tf.reshape(mols_batch, [S, B, 1, 3])\n",
    "    i_vec = tf.tile(i_vec, [1, 1, B, 1])\n",
    "\n",
    "    j_vec = tf.reshape(mols_batch, [S, 1, B, 3])\n",
    "    j_vec = tf.tile(j_vec, [1, B, 1, 1])\n",
    "\n",
    "    diff = tf.subtract(i_vec, j_vec)\n",
    "    temp = tf.eye(B, batch_shape=[S])\n",
    "    temp = tf.reshape(temp, [S, B, B, 1])\n",
    "    temp = tf.tile(temp, [1, 1, 1, 3])\n",
    "    diff = diff + temp\n",
    "    diff = diff / tf.sqrt(tf.reduce_sum(tf.square(diff), axis=3, keep_dims=True))\n",
    "    temp = tf.sqrt(tf.reduce_sum(tf.square(temp), axis=3, keep_dims=True))\n",
    "    diff = tf.subtract(diff, temp)\n",
    "\n",
    "    print(\"diff should be [S, B, B, 3]\")\n",
    "    print(diff)\n",
    "\n",
    "    ij = tf.tile(tf.reshape(diff, [S, B, 1, B, 3]), [1, 1, B, 1, 1])\n",
    "    ik = tf.tile(tf.reshape(diff, [S, B, B, 1, 3]), [1, 1, 1, B, 1])\n",
    "    dps = tf.reduce_sum(tf.multiply(ij ,ik), axis=4)\n",
    "    #dps = tf.where(tf.is_nan(dps), tf.ones_like(dps) * 0., dps)\n",
    "    print(\"dps should be [S, B, B, B]\")\n",
    "    print(dps)\n",
    "\n",
    "    #angles_gen = tf.acos(tf.clip_by_value(dps, -1.0, 1.0))\n",
    "    angles_computed = tf.where(tf.is_nan(dps), tf.zeros_like(dps), dps)\n",
    "    angles_computed = tf.multiply(angles_computed, angle_tuples_batch)\n",
    "    return(angles_computed)\n",
    "\n",
    "def compute_dist_tensor(mols_batch):\n",
    "    dist_matrix = tf.concat([tf.reshape(compute_distance_matrix(tf.reshape(mol, [B, 3])), (1, B, B)) for mol in tf.split(mols_batch, S, axis=0)], axis=0)\n",
    "    #dist_matrix = tf.where(tf.is_nan(dist_matrix), tf.ones_like(dist_matrix) * 0., dist_matrix)\n",
    "    print(\"dist_matrix\")\n",
    "    print(dist_matrix)\n",
    "    #dist_matrix = compute_distance_matrix(mol)\n",
    "    return(dist_matrix)\n",
    "\n",
    "\"\"\"\n",
    "def compute_dihedral_tensor(mols_batch, angle_tuples_batch):\n",
    "    i_vec = tf.reshape(mols_batch, [S, B, 1, 3])\n",
    "    i_vec = tf.tile(i_vec, [1, 1, B, 1])\n",
    "\n",
    "    j_vec = tf.reshape(mols_batch, [S, 1, B, 3])\n",
    "    j_vec = tf.tile(j_vec, [1, B, 1, 1])\n",
    "\n",
    "    diff = tf.subtract(i_vec, j_vec)\n",
    "    temp = tf.eye(B, batch_shape=[S])\n",
    "    temp = tf.reshape(temp, [S, B, B, 1])\n",
    "    temp = tf.tile(temp, [1, 1, 1, 3])\n",
    "    diff = diff + temp\n",
    "    diff = diff / tf.sqrt(tf.reduce_sum(tf.square(diff), axis=3, keep_dims=True))\n",
    "    temp = tf.sqrt(tf.reduce_sum(tf.square(temp), axis=3, keep_dims=True))\n",
    "    diff = tf.subtract(diff, temp)\n",
    "\n",
    "    print(\"diff should be [S, B, B, 3]\")\n",
    "    print(diff)\n",
    "\n",
    "    ij = tf.tile(tf.reshape(diff, [S, B, 1, B, 3]), [1, 1, B, 1, 1])\n",
    "    ik = tf.tile(tf.reshape(diff, [S, B, B, 1, 3]), [1, 1, 1, B, 1])\n",
    "    cross = tf.cross(ij, ik)\n",
    "    cross = cross / tf.sqrt(tf.reduce_sum(tf.square(cross), axis=4, keep_dims=True))\n",
    "    \n",
    "    ijk = tf.tile(tf.reshape(cross, [S, B, B, 1, B, 3]), [1m 1])\n",
    "    \n",
    "    dps = tf.reduce_sum(tf.multiply(ij ,ik), axis=4)\n",
    "    #dps = tf.where(tf.is_nan(dps), tf.ones_like(dps) * 0., dps)\n",
    "    print(\"dps should be [S, B, B, B]\")\n",
    "    print(dps)\n",
    "\n",
    "    #angles_gen = tf.acos(tf.clip_by_value(dps, -1.0, 1.0))\n",
    "    angles_computed = tf.where(tf.is_nan(dps), tf.zeros_like(dps), dps)\n",
    "    angles_computed = tf.multiply(angles_computed, angle_tuples_batch)\n",
    "    return(angles_computed)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(W_list, b_list, h_list, L_list, n_layers):\n",
    "    for layer_idx in range(n_layers):\n",
    "        h_list.append(tf.nn.dropout(adjacency_conv_layer(h_list[layer_idx], W_list[layer_idx],\n",
    "                                                     b_list[layer_idx], adj_list[layer_idx], \n",
    "                                                     clades_list[layer_idx],\n",
    "                                                     L_list[layer_idx], L_list[layer_idx+1],\n",
    "                                                     layer_idx, S, B_list[layer_idx]), keep_prob))\n",
    "    L_final = L_list[n_layers]  \n",
    "    \n",
    "    print(\"h_list[-1]\")\n",
    "    print(h_list[-1])\n",
    "    \n",
    "    print(\"B_list\")\n",
    "    print(B_list)\n",
    "    \n",
    "    fingerprints = tf.tile(tf.reshape(tf.nn.tanh(tf.matmul(tf.reduce_sum(h_list[-1], axis=1), f_w1) + f_b1), [S, 1, L_final]), [1, B, 1])\n",
    "    h_final = tf.concat([h_list[-1], fingerprints, z], axis=2)\n",
    "\n",
    "    \n",
    "    print(\"h afterz\")\n",
    "    print(h_final)\n",
    "    \n",
    "    g_h1 = tf.nn.dropout(tf.nn.tanh(tf.matmul(h_final, g_w1) + g_b1), keep_prob)\n",
    "    #g_h1 = tf.reshape(g_h1, [S, 5, 16])\n",
    "    print(\"g_h1\")\n",
    "    print(g_h1)\n",
    "    g_h2 = tf.reshape(tf.matmul(g_h1, g_w2) + g_b2, [S, B, 3])\n",
    "    #g_h2 = tf.clip_by_value(g_h2, -10., 10.)\n",
    "    return(g_h2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(W_list, b_list, h_list, L_list, n_layers, mols, mol_xyz):\n",
    "    mols = tf.add(mols, mol_noise)\n",
    "    print(\"mols\")\n",
    "    print(mols)\n",
    "    with tf.device('/gpu:0'):\n",
    "        for layer_idx in range(n_layers):\n",
    "            h_list.append(tf.nn.dropout(adjacency_conv_layer(h_list[layer_idx], W_list[layer_idx],\n",
    "                                             b_list[layer_idx], adj_list[layer_idx], \n",
    "                                             clades_list[layer_idx],\n",
    "                                             L_list[layer_idx], L_list[layer_idx+1],\n",
    "                                             layer_idx, S, B_list[layer_idx]), keep_prob))\n",
    "        h_final = h_list[-1]\n",
    "\n",
    "        #COMPUTE ANGLE LOSS:\n",
    "        angle_tensor = compute_angle_tensor(mol_xyz, angle_tuples)\n",
    "        angles_gen = compute_angle_tensor(mols, angle_tuples)\n",
    "\n",
    "        \n",
    "        wrong_angle_loss = tf.divide(tf.reshape(tf.multiply(tf.reduce_sum(tf.squared_difference(angles_gen, angle_tensor), axis=[1,2,3]), lam2), (S,1)), n_angles)\n",
    "        #wrong_angle_loss = tf.where(tf.is_nan(wrong_angle_loss), tf.zeros_like(wrong_angle_loss), wrong_angle_loss)\n",
    "\n",
    "        #wrong_angle_loss = tf.minimum(wrong_angle_loss, 3.14**2)\n",
    "        print(\"wrong_angle_loss\")\n",
    "        print(wrong_angle_loss)\n",
    "        \n",
    "        #COMPUTE BOND LOSS:\n",
    "        dist_matrix_gen = compute_dist_tensor(mols)\n",
    "        bond_dist_gen = tf.multiply(dist_matrix_gen, adj_matrix)\n",
    "        \n",
    "        dist_matrix_real = compute_dist_tensor(mol_xyz)\n",
    "        bond_dist_real = tf.multiply(dist_matrix_real, adj_matrix)\n",
    "\n",
    " \n",
    "        wrong_bond_loss = tf.divide(tf.reshape(tf.multiply(tf.reduce_sum(tf.square(tf.subtract(bond_dist_real, bond_dist_gen)), axis=[1,2]), lam1), (S,1)), n_bonds)\n",
    "        print(\"wrong_bond_loss\")\n",
    "        print(wrong_bond_loss)\n",
    "        \n",
    "        #COMPUTE CONTACT LOSS:\n",
    "        contact_dist = tf.multiply(dist_matrix_gen, non_adj_matrix)\n",
    "        contact_loss = tf.multiply(tf.reshape(tf.reduce_mean(tf.exp(tf.multiply(tf.multiply(tf.subtract(contact_dist, 2.), -1.), 3.)), axis=[1,2]), (S,1)), lam3)\n",
    "        \n",
    "        #COMPUTE DIHEDRAL LOSS?\n",
    "        \n",
    "        \n",
    "        feat_i = tf.reshape(h_final, (S, B, 1, L_list[n_layers]))\n",
    "        feat_i = tf.tile(feat_i, [1, 1, B, 1])\n",
    "        \n",
    "        feat_j = tf.reshape(h_final, (S, B, 1, L_list[n_layers]))\n",
    "        feat_j = tf.transpose(feat_j, perm=[0, 2, 1, 3])\n",
    "        feat_j = tf.tile(feat_j, [1, B, 1, 1])\n",
    "        \n",
    "        d_h1 = tf.concat([tf.reshape(dist_matrix_gen, [S, B, B, 1]), feat_i, feat_j], axis=3)\n",
    "        d_h1 = tf.reshape(d_h1, [S, B, B*(L_list[n_layers]*2+1)])\n",
    "        d_h2 = tf.nn.dropout(tf.nn.tanh(tf.matmul(d_h1, d_w1) + d_b1), keep_prob)\n",
    "        d_h2 = tf.divide(tf.reduce_sum(tf.matmul(d_h2, d_w2) + d_b2, axis=1), n_bonds)#, axis=1\n",
    "        print(\"d_h2\")\n",
    "        print(d_h2)\n",
    "        \n",
    "    \n",
    "    return(tf.add(tf.add(wrong_bond_loss, tf.multiply(wrong_angle_loss, 1.)), contact_loss), bond_dist_gen, angles_gen, wrong_bond_loss, wrong_angle_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "theta_D = DW_list[:n_layers] + Db_list[:n_layers] + [d_w1_ini, d_b1, d_w2_ini, d_b2] + [lam1, lam2, lam3]\n",
    "\n",
    "theta_G =  GW_list[:n_layers] + Gb_list[:n_layers] + [g_w1_ini, g_b1, g_w2_ini, g_b2, f_w1, f_b1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "with tf.device('/gpu:0'):    \n",
    "\n",
    "    G_sample = generator(GW_list, Gb_list, Gh_list, L_list, n_layers)\n",
    "\n",
    "    D_real, bond_dist_real, angles_gen_real, wrong_bond_real, wrong_angle_real = discriminator(DW_list, Db_list, Dh_list, L_list, n_layers, mol_xyz, mol_xyz)\n",
    "    D_fake, bond_dist_fake, angles_gen_fake, wrong_bond_fake, wrong_angle_fake = discriminator(DW_list, Db_list, Dh_list, L_list, n_layers, G_sample, mol_xyz)\n",
    "\n",
    "\n",
    "    D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\n",
    "    G_loss = -tf.reduce_mean(D_fake)\n",
    "    clip_D = []\n",
    "    for p in theta_D[:-2]:\n",
    "        #print(p)\n",
    "        clip_D.append(p.assign(tf.clip_by_value(p, -0.01, 0.01)))\n",
    "    clip_D.append(lam1.assign(tf.clip_by_value(lam1, 0.001, 0.002)))\n",
    "    clip_D.append(lam2.assign(tf.clip_by_value(lam2, 0.001, 0.002)))\n",
    "    clip_D.append(lam3.assign(tf.clip_by_value(lam3, 0.001, 0.002)))\n",
    "\n",
    "    \n",
    "    D_solver = (tf.train.RMSPropOptimizer(learning_rate=5e-3)\n",
    "                .minimize(-D_loss, var_list=theta_D))\n",
    "    G_solver = (tf.train.RMSPropOptimizer(learning_rate=5e-3)\n",
    "                .minimize(G_loss, var_list=theta_G))\n",
    "\n",
    "mb_size = S\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "preds = []\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "#print(sess.run(label_placeholder))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feed_dict(X, start=None,\n",
    "                      stop=None, y=None,\n",
    "                      keep_prob_val=1.0, train=True,\n",
    "                       shuffle_inds=True):\n",
    "    a = time.time()\n",
    "    if start is None:\n",
    "      start = 0\n",
    "      stop = len(X)\n",
    "    \n",
    "    inds = range(start, stop)\n",
    "    if shuffle_inds:\n",
    "        random.shuffle(inds)\n",
    "\n",
    "    atom_adj_batch = [X[idx][7][0] for idx in inds]\n",
    "    A_batch = np.array([X[idx][7][1] for idx in inds])\n",
    "    mol_xyz_batch = [X[idx][2] for idx in inds]\n",
    "    #mol_xyz_batch = rotate_molecules(mol_xyz_batch)\n",
    "\n",
    "    angle_tuples_batch = [X[idx][4] for idx in inds]\n",
    "    n_bonds_batch = np.reshape(np.array([X[idx][5] for idx in inds]), [S,1])\n",
    "    n_angles_batch = np.reshape(np.array([X[idx][6] for idx in inds]), [S,1])\n",
    "\n",
    "    feed_dict = {x: A_batch,\n",
    "                 adj_matrix: atom_adj_batch,\n",
    "                 mol_xyz: mol_xyz_batch,\n",
    "                 angle_tuples: angle_tuples_batch,\n",
    "                 n_bonds: n_bonds_batch,\n",
    "                 n_angles: n_angles_batch,\n",
    "                 keep_prob: keep_prob_val\n",
    "                }\n",
    "    t = time.time()-a\n",
    "    #print(\"Construct feed dict: %f\" %(t))\n",
    "    return(feed_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_molecule(mol_file, new_coords, new_file):\n",
    "    print(mol_file)\n",
    "    print(new_coords[:4])\n",
    "    if \".pdb\" in mol_file:\n",
    "        mol = Chem.MolFromPDBFile(mol_file)\n",
    "    else:\n",
    "        mol = Chem.MolFromMol2File(mol_file)\n",
    "    c = mol.GetConformer(0)\n",
    "    for i in range(mol.GetNumAtoms()):\n",
    "        c.SetAtomPosition(i, new_coords[i].tolist())\n",
    "    Chem.MolToMolFile(mol, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "every = 1000\n",
    "train_dataset = features[:64]\n",
    "n_train = len(train_dataset)\n",
    "\n",
    "for it in range(1000000):\n",
    "    a = time.time()\n",
    "    random.shuffle(train_dataset)\n",
    "    \n",
    "    batch_sched = list(range(0, n_train+1,S))\n",
    "    for j in range(0, len(batch_sched)-1):\n",
    "        start = batch_sched[j]\n",
    "        stop = batch_sched[j+1]\n",
    "        \n",
    "        for _ in range(5):\n",
    "            feed_dict = construct_feed_dict(train_dataset, start, stop)\n",
    "\n",
    "            _, D_loss_curr, _ = sess.run(\n",
    "                [D_solver, D_loss, clip_D], feed_dict=feed_dict)\n",
    "        \n",
    "        feed_dict = construct_feed_dict(train_dataset, start, stop)\n",
    "\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=feed_dict)\n",
    "        \n",
    "        d_losses.append(D_loss_curr)\n",
    "        g_losses.append(G_loss_curr)\n",
    "    \n",
    "    if it % every == 0:\n",
    "\n",
    "        print(time.time()-a)\n",
    "        print(\"Training epoch %d\" %it)\n",
    "                \n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        feed_dict = construct_feed_dict(features, 0, S, shuffle_inds=False, keep_prob_val=1.)\n",
    "\n",
    "        print(\"real:\")\n",
    "        print(feed_dict[mol_xyz][0][:4])\n",
    "        print(\"fake:\")\n",
    "        print(sess.run(G_sample, feed_dict=feed_dict)[0][:4])\n",
    "        \n",
    "        \n",
    "        print(\"bond real:\")\n",
    "        print(sess.run(bond_dist_real, feed_dict=feed_dict)[0][:4,:4])\n",
    "        #print(sess.run(computed_dist_real, feed_dict=feed_dict)[0][:4])\n",
    "\n",
    "        #print(feed_dict[bond_dist_mat][0][:4])\n",
    "\n",
    "\n",
    "        print(\"bond fake:\")\n",
    "        print(sess.run(bond_dist_fake, feed_dict=feed_dict)[0][:4,:4])\n",
    "    \n",
    "        #print(\"angles real:\")\n",
    "        #print(feed_dict[angle_tensor][0][2,:4,:4])\n",
    "        print(\"angles real computed\")\n",
    "        \n",
    "        print(sess.run(angles_gen_real, feed_dict=feed_dict)[0][2,:4,:4])\n",
    "        print(\"angles fake:\")\n",
    "        print(sess.run(angles_gen_fake, feed_dict=feed_dict)[0][2,:4,:4])\n",
    "        \n",
    "        print(\"bond loss real:\")\n",
    "        print(sess.run(wrong_bond_real, feed_dict=feed_dict)[:4])\n",
    "        print(\"bond loss fake:\")\n",
    "        print(sess.run(wrong_bond_fake, feed_dict=feed_dict))[:4] \n",
    "\n",
    "        print(\"angle loss real:\")\n",
    "        print(sess.run(wrong_angle_real, feed_dict=feed_dict)[:4])\n",
    "        print(\"angle loss fake:\")\n",
    "        print(sess.run(wrong_angle_fake, feed_dict=feed_dict)[:4])\n",
    "\n",
    "        print(\"lam1\")\n",
    "        print(sess.run(lam1, feed_dict=feed_dict))\n",
    "        print(\"lam2\")\n",
    "        print(sess.run(lam2, feed_dict=feed_dict))\n",
    "        \n",
    "        print(\"\\n\\n\\n\")\n",
    "        \n",
    "        generate_molecule(features[0][0], sess.run(G_sample, feed_dict=feed_dict)[0], \"/home/evan/Documents/deep_docking/dcgan_pg/DCGAN-tensorflow/generated_mols/%s_test%d.mol\" %(features[0][0].split(\".\")[0].split(\"/\")[-1], it))\n",
    "        generate_molecule(features[1][0], sess.run(G_sample, feed_dict=feed_dict)[1], \"/home/evan/Documents/deep_docking/dcgan_pg/DCGAN-tensorflow/generated_mols/%s_test%d.mol\" %(features[1][0].split(\".\")[0].split(\"/\")[-1], it))\n",
    "\n",
    "        \n",
    "        feed_dict = construct_feed_dict(features, len(features)-S, len(features), shuffle_inds=False, keep_prob_val=1.)\n",
    "        generate_molecule(features[-1][0], sess.run(G_sample, feed_dict=feed_dict)[0], \"/home/evan/Documents/deep_docking/dcgan_pg/DCGAN-tensorflow/generated_mols/%s_test%d.mol\" %(features[-1][0].split(\".\")[0].split(\"/\")[-1], it))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #print(\"output_i\")\n",
    "        #print(sess.run(G_sample_output_i, feed_dict=feed_dict))\n",
    "        #print(sess.run(G_sample, feed_dict=feed_dict))\n",
    "\n",
    "    #random.shuffle(train_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feed_dict = construct_feed_dict(features, 0, S, shuffle_inds=False)\n",
    "\n",
    "sess.run(G_sample, feed_dict=feed_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = construct_feed_dict(features, len(features)-S, len(features), shuffle_inds=False)\n",
    "generate_molecule(features[-1][0], sess.run(G_sample, feed_dict=feed_dict)[0], \"/home/evan/Documents/deep_docking/dcgan_pg/DCGAN-tensorflow/generated_mols/%s_test%d.mol\" %(features[-1][0].split(\".\")[0].split(\"/\")[-1], it))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.min(results[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(features[0][1][:,:,:,:-1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def discriminator(W_list, b_list, h_list, L_list, n_layers, mols):\n",
    "    mols = tf.add(mols, mol_noise)\n",
    "    with tf.device('/gpu:0'):\n",
    "        for layer_idx in range(n_layers):\n",
    "            h_list.append(tf.nn.dropout(adjacency_conv_layer(h_list[layer_idx], W_list[layer_idx],\n",
    "                                             b_list[layer_idx], adj_list[layer_idx], \n",
    "                                             clades_list[layer_idx],\n",
    "                                             L_list[layer_idx], L_list[layer_idx+1],\n",
    "                                             layer_idx, S, B_list[layer_idx]), keep_prob))\n",
    "        h_final = h_list[-1]\n",
    "        dist_matrix = tf.concat([tf.reshape(compute_distance_matrix(tf.reshape(mol, [B, 3])), (1, B, B)) for mol in tf.split(mols, S, axis=0)], axis=0)\n",
    "        print(\"dist_matrix\")\n",
    "        print(dist_matrix)\n",
    "        #dist_matrix = compute_distance_matrix(mol)\n",
    "        bond_dist_gen = tf.multiply(dist_matrix, adj_matrix)\n",
    "        wrong_bond_loss = tf.divide(tf.reshape(tf.multiply(tf.reduce_sum(tf.square(tf.subtract(bond_dist_mat, bond_dist_gen)), axis=[1,2]), lam1), (S,1)), n_bonds)\n",
    "        print(\"wrong_bond_loss\")\n",
    "        print(wrong_bond_loss)\n",
    "\n",
    "        angles = []\n",
    "        for i in range(0, B):\n",
    "            for j in range(0, B):\n",
    "                ij = mols[:, i,:] - mols[:,j,:]\n",
    "                ij = ij / tf.sqrt(tf.reduce_sum(tf.square(ij), 1, keep_dims=True))\n",
    "                #ij shape should be (S, 3)\n",
    "               # print(\"ij\")\n",
    "                #print(ij)\n",
    "                for k in range(0,B):\n",
    "                    ik = mols[:,i,:] - mols[:,k,:]\n",
    "                    ik = ik / tf.sqrt(tf.reduce_sum(tf.square(ik), 1, keep_dims=True))\n",
    "\n",
    "                    #ik shape should be (S, 3)\n",
    "                    #if i == 0 and j == 0 and k == 0: \n",
    "                    #   print(\"ik\")\n",
    "                     #   print(ik)\n",
    "                        \n",
    "                    dp = tf.reduce_sum(tf.multiply(ij, ik), axis=1, keep_dims=True)\n",
    "                    #dp shape should be (S,1)\n",
    "                    #if i ==0 and j ==0 and k==0:\n",
    "                    #    print(\"dp\")\n",
    "                    #    print(dp)\n",
    "                    if i == j or i == k or j == k:\n",
    "                        angle = tf.Variable(tf.constant(np.zeros([4,1]).astype(np.float32)))\n",
    "                    else:\n",
    "                        angle = tf.acos(tf.clip_by_value(tf.reduce_sum(tf.multiply(ij, ik), axis=1, keep_dims=True), -1.0, 1.0))\n",
    "                    #if i ==0 and j==0 and k==0:\n",
    "                        #print(\"angle\")\n",
    "                        #print(angle)\n",
    "                    angles.append(angle)\n",
    "        angles_gen = tf.reshape(tf.concat(angles, axis=1), [S, B, B, B])\n",
    "        angles_gen = tf.multiply(angles_gen, angle_tuples)\n",
    "        wrong_angles_loss = tf.divide(tf.reshape(tf.multiply(tf.reduce_sum(tf.square(tf.subtract(angle_tensor, angles_gen)), axis=[1,2,3]), lam2), (S,1)), n_angles)\n",
    "                \n",
    "        feat_i = tf.reshape(h_final, (S, B, 1, L_list[n_layers]))\n",
    "        feat_i = tf.tile(feat_i, [1, 1, B, 1])\n",
    "        \n",
    "        feat_j = tf.reshape(h_final, (S, B, 1, L_list[n_layers]))\n",
    "        feat_j = tf.transpose(feat_j, perm=[0, 2, 1, 3])\n",
    "        feat_j = tf.tile(feat_j, [1, B, 1, 1])\n",
    "        \n",
    "        d_h1 = tf.concat([tf.reshape(dist_matrix, [S, B, B, 1]), feat_i, feat_j], axis=3)\n",
    "        d_h1 = tf.reshape(d_h1, [S, B, B*(L_list[n_layers]*2+1)])\n",
    "        d_h2 = leaky_relu(tf.matmul(d_h1, d_w1) + d_b1)\n",
    "        d_h2 = tf.divide(tf.reduce_sum(tf.matmul(d_h2, d_w2) + d_b2, axis=1), n_bonds)#, axis=1\n",
    "        print(\"d_h2\")\n",
    "        print(d_h2)\n",
    "        \n",
    "    \n",
    "    return(wrong_bond_loss + wrong_angles_loss + d_h2, bond_dist_gen, angles_gen, wrong_bond_loss, wrong_angles_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mol = Chem.MolFromMol2File(ligand_files[0])\n",
    "c = mol.GetConformer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.SetAtomPosition(0, [0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(features[2][1][:,:,:,:-1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = sess.run(G_sample, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(pred_real, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.max(results[0][:,:,:,:], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mol = Chem.MolFromMol2File(features[1][0])\n",
    "mol.GetNumAtoms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rdkit.Chem import AllChem\n",
    "AllChem.Compute2DCoords(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "\n",
    "Draw.MolToImage(mol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
