{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook addresses the question, \"Can we represent a molecule as a graph via a 1D column vector or a 2D matrix of fixed length, with maximum number of atoms n_rows?\" Then, can we use this representation to learn neural fingerprints? E.g., can we make an aromatic ring detector? \n",
    "\n",
    "Scheme:\n",
    "feature_matrix = X\n",
    "for each ligand:\n",
    "    choose a central atom. this can be the atom (node) that minimizes distance to furthest heavy atom in graph.\n",
    "    set first row of X to be this central atom\n",
    "    set next four rows to be the atoms bonded to that centrl atom\n",
    "        set zeros for rows where row ind > n_bonds of atom\n",
    "    for each of those atoms:\n",
    "        repeat. find their neighbors. add to matrix.\n",
    "\n",
    "algorithm: breadth-first search:\n",
    "1. create networkx graph based on molecule\n",
    "2. find \"central\" atom (different strategies)\n",
    "3. define atom matrix of size (1+4+4*3^(L-1)) x (n_features_per_atom)\n",
    "4. start atom queue q\n",
    "5. central_atom.layer = 0; central_atom.row_idx = 0;\n",
    "6. q.enqueue(central_atom)\n",
    "7. define adjacency matrix of size (1+4+4*3^(L-1)) x 4\n",
    "\n",
    "def get_row_idx(curr_layer, prev_row_idx, curr_neighbor_idx):\n",
    "    if curr_layer == 0:\n",
    "        return(0)\n",
    "    if curr_layer == 1:\n",
    "        row_idx = 1 + curr_neighbor_idx\n",
    "    if layer == 2:\n",
    "        last_max = 5\n",
    "        row_idx = last_max + (3*(prev_row_idx-last_max)) + curr_neighbor_idx\n",
    "    if layer > 2:\n",
    "        last_max = 5 + 4*3^(curr_layer-2) \n",
    "        row_idx = last_max + 3*(prev_row_idx-last_max) + curr_neighbor_idx\n",
    "    return(row_idx)\n",
    "    \n",
    "\n",
    "while q.is_not_empty():\n",
    "    a = q.dequeue()\n",
    "    a.visited = True\n",
    "    for n_idx, n in enumerate(a.neighbors()):\n",
    "        if not n.visited:\n",
    "            row_idx = c\n",
    "            n.layer = a.layer + 1\n",
    "            row_idx = get_row_idx(n.layer, a.row_idx, n_idx)\n",
    "            n.row_idx = row_idx\n",
    "            adj_matrix[a.row_idx][n_idx] = n.row_idx\n",
    "            atom_matrix[row_idx][elem_to_idx[n.elem]] = 1\n",
    "\n",
    "input_matrix = tf.concat([atom_matrix, atom_matrix[adj_matrix[:,0]], atom_matrix[adj_matrix[:,1]], atom_matrix[adj_matrix[:,2]], atom_matrix[adj_matrix[:,3]]\n",
    "\n",
    "neural net:\n",
    "h1 = relu([tf.zeros([n_features_per_atom, 4]) * input_matrix + bias))\n",
    "h1_conc = tf.concat([h1, h1[adj_matrix[:,0], ..., h1[adj_matrix[:,3])\n",
    "\n",
    "repeat h1 to get h2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dihedral predictor pseudocode:\n",
    "\n",
    "get bonds for molecule\n",
    "create networkx graph out of molecule (use atom indices)\n",
    "\n",
    "for each edge:\n",
    "   for neighbor_i in atom_i.neighbors():\n",
    "       if neighbor_i == atom_j: continue\n",
    "       for neighbor_j in atom_j.neighbors():\n",
    "           if neighbor_j == atom_i: continue\n",
    "           dihedrals.append((neighbor_i, atom_i, neighbor_j, atom_j))\n",
    "           check to make sure (atom_j, neighbor_j, atom_i, neighbor_i)) not already in list\n",
    "\n",
    "for dihedral in dihedrals:\n",
    "    angle =  rdMolTransforms.GetDihedralDeg(c, 0,1,2,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, rdMolTransforms\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.mixture import GMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_torsions_angles(mol):\n",
    "    c = mol.GetConformer(0)\n",
    "\n",
    "    torsions = []\n",
    "    torsion_tuples = []\n",
    "    \n",
    "    torsion_matrix = np.zeros((100,3))\n",
    "    torsion_indices = np.zeros((100,200,12)).astype(np.uint8)\n",
    "    bond_idx = 0\n",
    "    for idx, bond in enumerate(mol.GetBonds()):\n",
    "        atom_i = bond.GetBeginAtom()\n",
    "        atom_j = bond.GetEndAtom()\n",
    "        if bond.IsInRing(): \n",
    "            continue\n",
    "        \n",
    "        exist_dihed = False\n",
    "        for neighbor_j in atom_j.GetNeighbors():\n",
    "            if neighbor_j.GetIdx() == atom_i.GetIdx():\n",
    "                continue\n",
    "            \n",
    "            dihed_idx = 0\n",
    "            for neighbor_i in atom_i.GetNeighbors():\n",
    "                if neighbor_i.GetIdx() == atom_j.GetIdx():\n",
    "                    continue\n",
    "                \n",
    "                exist_dihed=True\n",
    "                torsion_tuple = (neighbor_i.GetIdx(), atom_i.GetIdx(), atom_j.GetIdx(), neighbor_j.GetIdx())\n",
    "\n",
    "                torsion_matrix[bond_idx][dihed_idx] = np.cos(rdMolTransforms.GetDihedralRad(c, *torsion_tuple))\n",
    "                torsion_indices[bond_idx][torsion_tuple[0]][dihed_idx*4] = 1\n",
    "                torsion_indices[bond_idx][torsion_tuple[1]][dihed_idx*4+1] = 1\n",
    "                torsion_indices[bond_idx][torsion_tuple[2]][dihed_idx*4+2] = 1\n",
    "                torsion_indices[bond_idx][torsion_tuple[3]][dihed_idx*4+3] = 1\n",
    "                dihed_idx += 1\n",
    "            #torsion_matrix[bond_idx][3] = 3.14159 - np.abs(np.abs(torsion_matrix[bond_idx][2]-torsion_matrix[bond_idx][1])-3.14159)\n",
    "            #torsion_matrix[bond_idx][4] = 3.14159 - np.abs(np.abs(torsion_matrix[bond_idx][2]-torsion_matrix[bond_idx][0])-3.14159)\n",
    "            #torsion_matrix[bond_idx][5] = 3.14159 - np.abs(np.abs(torsion_matrix[bond_idx][1]-torsion_matrix[bond_idx][0])-3.14159)\n",
    "            break\n",
    "        if exist_dihed:\n",
    "            bond_idx += 1\n",
    "            break\n",
    "    return((torsion_indices, csr_matrix(torsion_matrix)))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_mols(mol_files):\n",
    "    featurizer = AdjacencyFingerprint(max_n_atoms=200)\n",
    "    features = []\n",
    "    for idx, mol_file in enumerate(mol_files):\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "        if 1==1:\n",
    "            mol = Chem.MolFromMol2File(mol_file)\n",
    "            if mol is None:\n",
    "                features.append(None)\n",
    "                continue\n",
    "            torsions = get_torsions_angles(mol)\n",
    "            graph_feat = featurizer.featurize([mol])[0]\n",
    "            features.append((mol_file, torsions, graph_feat))\n",
    "        else:\n",
    "            features.append(None)\n",
    "    return(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.feat.graph_features import ConvMolFeaturizer\n",
    "from deepchem.feat.adjacency_fingerprints import AdjacencyFingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "feature_file = \"./dihedral_features_pdbbind.pkl\"\n",
    "#if not os.path.exists(feature_file):\n",
    "if 1== 1:\n",
    "    pdbbind_dir = \"/home/evan/Documents/deep_docking/datasets/v2015/\"\n",
    "    def find_files(directory, pattern):\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for basename in files:\n",
    "                if fnmatch.fnmatch(basename, pattern):\n",
    "                    filename = os.path.join(root, basename)\n",
    "                    yield filename\n",
    "    ligand_files = [f for f in find_files(pdbbind_dir, \"*ligand.mol2\")][:1]\n",
    "    features = featurize_mols(ligand_files)\n",
    "    with open(feature_file, \"wb\") as f:\n",
    "        pickle.dump(features, f, protocol=2)\n",
    "else:\n",
    "    with open(feature_file, \"rb\") as f:\n",
    "        features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [f for f in features if f is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61143659, -0.99900387,  0.48252666],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0][1][1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "alpha = 0.01\n",
    "n_layers = 2\n",
    "S = 32\n",
    "B = 200\n",
    "L_list = [50, 50, 50, 50]\n",
    "p = 75\n",
    "n_bonds=100\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[S, B, p])\n",
    "\n",
    "non_zero_inds = tf.placeholder(tf.int32, shape=[None, S*n_bonds])\n",
    "\n",
    "adj_matrix = tf.placeholder(tf.float32, shape=[S, B, B])\n",
    "dihed_indices = tf.placeholder(tf.float32, shape=[S, n_bonds, B, 12])\n",
    "\n",
    "label_placeholder = tf.placeholder(\n",
    "    dtype='float32', shape=[S*n_bonds,3], name=\"label_placeholder\")\n",
    "\n",
    "phase = tf.placeholder(dtype='bool', name='phase')\n",
    "\n",
    "z = tf.placeholder(tf.float32,\n",
    "                          [None, 200, 75], name='z')\n",
    "\n",
    "z_ini = tf.placeholder(tf.float32,\n",
    "                          [None, 200, 75], name='z_ini')\n",
    "\n",
    "ones_like = tf.placeholder(tf.float32, shape=[None,1])\n",
    "zeros_like = tf.placeholder(tf.float32, shape=[None,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DISCRIMINATOR\n",
    "\n",
    "D_W_list = [None for i in range(n_layers)]\n",
    "D_b_list = [None for i in range(n_layers)]\n",
    "D_h_list = [None for i in range(n_layers)]\n",
    "\n",
    "\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    if layer_idx == 0:\n",
    "      L_in = p\n",
    "      L_out = L_list[0]\n",
    "      atom_matrix = x\n",
    "    else:\n",
    "      L_in = L_list[layer_idx-1]\n",
    "      L_out = L_list[layer_idx]\n",
    "      atom_matrix = D_h_list[layer_idx-1]\n",
    "\n",
    "    D_W_list[layer_idx] = tf.Variable(tf.truncated_normal([L_in, L_out], seed=2017), name=\"D_W_list%d\" %layer_idx)\n",
    "    D_b_list[layer_idx] = tf.Variable(tf.zeros([1, L_out]))\n",
    "\n",
    "    \n",
    "D_W2 = tf.Variable(tf.truncated_normal([L_list[-1]*12+3, 100]))\n",
    "D_b2 = tf.Variable(tf.zeros([1, 100]))\n",
    "\n",
    "D_W3 = tf.Variable(tf.truncated_normal([100, 3]))\n",
    "D_b3 = tf.Variable(tf.zeros([1, 3]))\n",
    "\n",
    "D_W4 = tf.Variable(tf.truncated_normal([3, 10]))\n",
    "D_b4 = tf.Variable(tf.zeros([1, 10]))\n",
    "\n",
    "D_W5 = tf.Variable(tf.truncated_normal([10, 1]))\n",
    "D_b5 = tf.Variable(tf.zeros([1, 1]))\n",
    "\n",
    "def adjacency_conv_layer(atom_matrix, W, b, L_in, L_out, layer_idx, is_training=True):\n",
    "    print(\"layer_idx: %d\" %(layer_idx))\n",
    "    h = tf.matmul(adj_matrix, atom_matrix, name=\"adj_conv1\")\n",
    "    h = tf.reshape(h, shape=(S*B, L_in))\n",
    "\n",
    "    h = tf.matmul(h, W) + b\n",
    "    h = tf.maximum(alpha*h, h)\n",
    "    \n",
    "    h = tf.reshape(h, (S, B, L_out))\n",
    "    #h = tf.contrib.layers.batch_norm(h, \n",
    "    #                                    center=True, scale=False, \n",
    "    #                                    is_training=is_training)\n",
    "\n",
    "    return(h)\n",
    "\n",
    "def discriminator(angles, is_training=True):\n",
    "    for layer_idx in range(n_layers):\n",
    "        if layer_idx == 0:\n",
    "          L_in = p\n",
    "          L_out = L_list[0]\n",
    "          atom_matrix = x\n",
    "        else:\n",
    "          L_in = L_list[layer_idx-1]\n",
    "          L_out = L_list[layer_idx]\n",
    "          atom_matrix = D_h_list[layer_idx-1]\n",
    "        \n",
    "        D_h_list[layer_idx] = adjacency_conv_layer(atom_matrix, D_W_list[layer_idx], D_b_list[layer_idx], L_in, L_out, layer_idx)\n",
    "\n",
    "    L_final = L_list[n_layers-1]\n",
    "    h_final = tf.reshape(D_h_list[layer_idx], (S, B, L_final))\n",
    "\n",
    "    #add dihedral regressor layers\n",
    "\n",
    "    d0 = []\n",
    "    for i in range(0, S):\n",
    "        mol_tuple = []\n",
    "        for j in range(0, 12):\n",
    "          entry = h_final[i]\n",
    "          indices = dihed_indices[i][:,:,j]\n",
    "          atom_list = tf.matmul(indices, entry, name=\"disc1\")\n",
    "          atom_list = tf.reshape(atom_list, (n_bonds, L_final))\n",
    "          mol_tuple.append(atom_list)\n",
    "        mol_tuple = tf.reshape(tf.stack(mol_tuple, axis=1), (n_bonds, L_final*12))\n",
    "        d0.append(mol_tuple)\n",
    "\n",
    "    d0 = tf.concat(d0, axis=0)\n",
    "    d0 = tf.concat([d0, angles], axis=1)\n",
    "    d0 = tf.matmul(tf.cast(non_zero_inds, tf.float32), d0)\n",
    "\n",
    "\n",
    "    d2 = tf.matmul(d0, D_W2, name=\"disc2\") + D_b2\n",
    "    d2 = tf.maximum(d2, alpha*d2)\n",
    "\n",
    "    #d2 = tf.contrib.layers.batch_norm(d2, \n",
    "    #                                    center=True, scale=False, \n",
    "    #                                    is_training=is_training)\n",
    "    \n",
    "    d3 = tf.matmul(d2, D_W3, name=\"disc3\") + D_b3\n",
    "    d3 = tf.maximum(d3, alpha*d3)\n",
    "    \n",
    "    #d3 = tf.nn.tanh(d3)\n",
    "\n",
    "    #d3 = tf.contrib.layers.batch_norm(d3, \n",
    "    #                                    center=True, scale=False, \n",
    "    #                                    is_training=is_training)\n",
    "    \n",
    "                                                                                                                                                               \n",
    "    d4 = tf.matmul(d3, D_W4, name=\"disc3\") + D_b4\n",
    "    \n",
    "    output = tf.matmul(d4, D_W5, name=\"disc4\") + D_b5\n",
    "    \n",
    "    #D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def atan2(y, x, epsilon=1.0e-12):\n",
    "  # Add a small number to all zeros, to avoid division by zero:\n",
    "  x = tf.where(tf.equal(x, 0.0), x+epsilon, x)\n",
    "  y = tf.where(tf.equal(y, 0.0), y+epsilon, y)\n",
    "\n",
    "  angle = tf.where(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))\n",
    "  angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n",
    "  angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n",
    "  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n",
    "  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n",
    "  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), tf.zeros_like(x), angle)\n",
    "  return angle\n",
    "\n",
    "#GENERATOR\n",
    "\n",
    "G_W_list = [None for i in range(n_layers)]\n",
    "G_b_list = [None for i in range(n_layers)]\n",
    "G_h_list = [None for i in range(n_layers)]\n",
    "\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    if layer_idx == 0:\n",
    "      L_in = p\n",
    "      L_out = L_list[0]\n",
    "      atom_matrix = x\n",
    "    else:\n",
    "      L_in = L_list[layer_idx-1]\n",
    "      L_out = L_list[layer_idx]\n",
    "      atom_matrix = G_h_list[layer_idx-1]\n",
    "\n",
    "    G_W_list[layer_idx] = tf.Variable(tf.truncated_normal([L_in, L_out], seed=2017), name=\"G_W_list%d\" %layer_idx)\n",
    "    G_b_list[layer_idx] = tf.Variable(tf.zeros([1, L_out]))\n",
    "\n",
    "    \n",
    "G_W2 = tf.Variable(tf.truncated_normal([L_list[-1]*12, 100]))\n",
    "G_b2 = tf.Variable(tf.zeros([1, 100]))\n",
    "\n",
    "G_W3 = tf.Variable(tf.truncated_normal([100, 100]))\n",
    "G_b3 = tf.Variable(tf.zeros([1, 100]))\n",
    "\n",
    "G_W4 = tf.Variable(tf.truncated_normal([100, 3]))\n",
    "G_b4 = tf.Variable(tf.zeros([1, 3]))\n",
    "\n",
    "\n",
    "def gen_adjacency_conv_layer(atom_matrix, W, b, L_in, L_out, layer_idx, z, is_training):\n",
    "    print(\"layer_idx: %d\" %(layer_idx))\n",
    "    h = tf.matmul(adj_matrix, atom_matrix, name=\"gen0_%d\" %layer_idx)\n",
    "    h = tf.reshape(h, shape=(S*B, L_in))\n",
    "\n",
    "    h = tf.matmul(h, W, name=\"gen1\") + b\n",
    "    h = tf.maximum(alpha*h, h)\n",
    "\n",
    "    \n",
    "    h = tf.reshape(h, (S, B, L_out))\n",
    "    #h = tf.contrib.layers.batch_norm(h, \n",
    "    #                                    center=True, scale=False, \n",
    "    #                                    is_training=is_training)\n",
    "\n",
    "    return(h)\n",
    "\n",
    "def generator(z, is_training=True):\n",
    "    for layer_idx in range(n_layers):\n",
    "        if layer_idx == 0:\n",
    "          L_in = p\n",
    "          L_out = L_list[0]\n",
    "          atom_matrix = tf.add(x, z)\n",
    "        else:\n",
    "          L_in = L_list[layer_idx-1]\n",
    "          L_out = L_list[layer_idx]\n",
    "          atom_matrix = G_h_list[layer_idx-1]\n",
    "        \n",
    "        G_h_list[layer_idx] = gen_adjacency_conv_layer(atom_matrix, G_W_list[layer_idx], G_b_list[layer_idx], L_in, L_out, layer_idx, z, is_training)\n",
    "\n",
    "    L_final = L_list[n_layers-1]\n",
    "    g_h_final = tf.reshape(G_h_list[layer_idx], (S, B, L_final))\n",
    "\n",
    "    #add dihedral regressor layers\n",
    "\n",
    "    g_d0 = []\n",
    "    for i in range(0, S):\n",
    "        mol_tuple = []\n",
    "        for j in range(0, 12):\n",
    "          entry = g_h_final[i]\n",
    "          indices = dihed_indices[i][:,:,j]\n",
    "          atom_list = tf.matmul(indices, entry, name='gen2')\n",
    "          atom_list = tf.reshape(atom_list, (n_bonds, L_final))\n",
    "          mol_tuple.append(atom_list)\n",
    "        mol_tuple = tf.reshape(tf.stack(mol_tuple, axis=1), (n_bonds, L_final*12))\n",
    "        g_d0.append(mol_tuple)\n",
    "\n",
    "    g_d0 = tf.concat(g_d0, axis=0)\n",
    "    \n",
    "    g_d2 = tf.matmul(g_d0, G_W2, name='gen3') + G_b2\n",
    "    g_d2 = tf.maximum(alpha*g_d2, g_d2)\n",
    "\n",
    "    \n",
    "    #g_d2 = tf.contrib.layers.batch_norm(g_d2, \n",
    "    #                                    center=True, scale=False, \n",
    "    #                                    is_training=is_training)\n",
    "    \n",
    "    g_d3 = tf.matmul(g_d2, G_W3) + G_b3\n",
    "    g_d3 = tf.nn.tanh(g_d3)\n",
    "\n",
    "    #g_d3 = tf.contrib.layers.batch_norm(g_d3, \n",
    "    #                                    center=True, scale=False, \n",
    "    #                                    is_training=is_training)\n",
    "                                                                                                                                                               \n",
    "    g_d4 = tf.matmul(g_d3, G_W4) + G_b4\n",
    "    \n",
    "    output = tf.cos(g_d4)\n",
    "    #d3_cos = tf.cos(g_d4)\n",
    "    #d3_sin = tf.sin(g_d4)\n",
    "    #output = atan2(d3_sin, d3_cos)\n",
    "    \n",
    "    #output = tf.concat([output,\n",
    "    #                    tf.reshape(3.14159 - tf.abs(tf.abs(output[:,2]-output[:,1])-3.14159), (-1,1)),\n",
    "    #                    tf.reshape(3.14159 - tf.abs(tf.abs(output[:,2]-output[:,0])-3.14159), (-1,1)),\n",
    "    #                    tf.reshape(3.14159 - tf.abs(tf.abs(output[:,1]-output[:,0])-3.14159), (-1,1))],\n",
    "    #                   axis=1)\n",
    "    print(\"output\")\n",
    "    print(output)\n",
    "    \n",
    "    G_logit = tf.nn.sigmoid(output)\n",
    "\n",
    "    return(output, G_logit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_D = [D_W_list[0], D_W_list[1], D_W2, D_W3, D_W4, D_W5, D_b_list[0], D_b_list[1], D_b2, D_b3, D_b4, D_b5]\n",
    "\n",
    "theta_G = [G_W_list[0], G_W_list[1], G_W2, G_W3, G_W4, G_b_list[0], G_b_list[1], G_b2, G_b3, G_b4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm = GMM(2, n_iter=1., covariance_type=\"spherical\")\n",
    "\n",
    "gmm.means_ = np.array([[-10], [10]])\n",
    "gmm.covars_ = np.array([[0.0001], [0.0001]])\n",
    "\n",
    "def construct_feed_dict(X, start=None,\n",
    "                      stop=None, y=None,\n",
    "                      keep_prob=1.0, train=False):\n",
    "    if start is None:\n",
    "      start = 0\n",
    "      stop = len(X)\n",
    "\n",
    "    adj = [X[idx][2][0].toarray().astype(np.float32) for idx in range(start, stop)]\n",
    "    A_batch = [X[idx][2][1].toarray() for idx in range(start, stop)]\n",
    "    D_batch = [X[idx][1][0] for idx in range(start, stop)]\n",
    "    y_batch = [X[idx][1][1].toarray() for idx in range(start, stop)]\n",
    "\n",
    "    y_batch = np.concatenate(y_batch, axis=0)\n",
    "    non_zero_batch = np.where(y_batch[:,0] != 0.)[0]\n",
    "    \n",
    "    ones_batch = np.random.uniform(0.7, 1.2, size=(len(non_zero_batch),1))\n",
    "    zeros_batch = np.random.uniform(0., 0.3, size=(len(non_zero_batch),1))\n",
    "\n",
    "    #y_batch = gmm.sample(len(y_batch))\n",
    "    \n",
    "    y_batch = y_batch + np.random.normal(0, 0.03, size=y_batch.shape)\n",
    "    \n",
    "    #y_batch = np.squeeze(gmm.sample(len(y_batch)))\n",
    "    \n",
    "    #adj = [np.random.uniform(-1., 1., size=a.shape) for a in adj]\n",
    "    #A_batch = [np.random.uniform(-1., 1., size=a.shape) for a in A_batch]\n",
    "    #D_batch = [np.random.uniform(-1., 1., size=a.shape) for a in D_batch]\n",
    "\n",
    "\n",
    "    onehotter = OneHotEncoder(n_values = S*X[0][1][1].shape[0])\n",
    "    non_zero_onehot = onehotter.fit_transform(non_zero_batch).toarray().reshape((len(non_zero_batch),S*X[0][1][1].shape[0]))\n",
    "    \n",
    "    #z_batch = np.zeros((S, adj[0].shape[0], 75))\n",
    "    z_batch = np.random.uniform(-0.1, 0.1, size=(S, adj[0].shape[0],75))\n",
    "    #y_batch = np.random.random(size=(S*250))\n",
    "    \n",
    "    feed_dict = {x: A_batch,\n",
    "                 adj_matrix: adj,\n",
    "                 phase: train,\n",
    "                 label_placeholder: y_batch,\n",
    "                 non_zero_inds: non_zero_onehot,\n",
    "                 dihed_indices: D_batch,\n",
    "                 z:z_batch,\n",
    "                 ones_like: ones_batch,\n",
    "                 zeros_like: zeros_batch\n",
    "                }\n",
    "    return(feed_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx: 0\n",
      "layer_idx: 1\n",
      "output\n",
      "Tensor(\"Cos:0\", shape=(3200, 3), dtype=float32)\n",
      "layer_idx: 0\n",
      "layer_idx: 1\n",
      "output\n",
      "Tensor(\"Cos_1:0\", shape=(3200, 3), dtype=float32)\n",
      "layer_idx: 0\n",
      "layer_idx: 1\n",
      "layer_idx: 0\n",
      "layer_idx: 1\n",
      "Training epoch 0\n",
      "Iter: 0\n",
      "D loss: 1.018e-07\n",
      "G_loss: 6.264e-08\n",
      "()\n",
      "[-0.01263064  0.00209611  0.12713955]\n",
      "Training epoch 100\n",
      "Iter: 100\n",
      "D loss: -1.649e-05\n",
      "G_loss: 3.973e-06\n",
      "()\n",
      "[-0.08445911  0.15667461  0.06386196]\n",
      "Training epoch 200\n",
      "Iter: 200\n",
      "D loss: -2.511e-05\n",
      "G_loss: 7.714e-06\n",
      "()\n",
      "[ 0.06070056  0.19933181 -0.0068552 ]\n"
     ]
    }
   ],
   "source": [
    "G_pred, G_pred_logit = generator(z, is_training=False)\n",
    "G_sample, G_sample_logit = generator(z, is_training=True)\n",
    "\n",
    "D_real  = discriminator(label_placeholder)\n",
    "D_fake  = discriminator(G_sample)\n",
    "\n",
    "# D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "# G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# Alternative losses:\n",
    "# -------------------\n",
    "\n",
    "#D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=ones_like))\n",
    "#D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=zeros_like))\n",
    "#D_loss = D_loss_real + D_loss_fake\n",
    "#G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=ones_like))\n",
    "\n",
    "#D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "#D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "#D_loss = D_loss_real + D_loss_fake\n",
    "#G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "#WGAN:\n",
    "D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\n",
    "G_loss = -tf.reduce_mean(D_fake)\n",
    "clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mb_size = S\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_dataset = [features[0] for i in range(0,32)]\n",
    "n_train = len(train_dataset)\n",
    "preds = []\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "for it in range(100000):\n",
    "    if it % 100 == 0:\n",
    "        print(\"Training epoch %d\" %it)\n",
    "    batch_sched = list(range(0, n_train+1,S))\n",
    "    for j in range(0, len(batch_sched)-1):\n",
    "        start = batch_sched[j]\n",
    "        stop = batch_sched[j+1]\n",
    "        feed_dict = construct_feed_dict(train_dataset, start, stop)\n",
    "\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict=feed_dict)\n",
    "        \n",
    "        for _ in range(5):\n",
    "            feed_dict = construct_feed_dict(train_dataset, start, stop)\n",
    "\n",
    "            _, D_loss_curr, _ = sess.run(\n",
    "                [D_solver, D_loss, clip_D],\n",
    "                feed_dict=feed_dict)\n",
    "            \n",
    "        \n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=feed_dict)\n",
    "        \n",
    "        d_losses.append(D_loss_curr)\n",
    "        g_losses.append(G_loss_curr)\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        samples = []\n",
    "        for trial in range(0,100):\n",
    "            feed_dict = construct_feed_dict(train_dataset, start, stop)\n",
    "            samples.append(np.reshape(sess.run(G_sample, feed_dict=feed_dict)[0], (1,3)))\n",
    "        samples = np.concatenate(samples, axis=0)\n",
    "        print(np.mean(samples, axis=0)[:10])\n",
    "        if it > 999:\n",
    "            plt.plot(range(0,it+1,1), d_losses)\n",
    "            plt.show()\n",
    "            plt.plot(range(0, it+1, 1), g_losses)\n",
    "            plt.show()\n",
    "        #print(samples[:10])\n",
    "        #plt.hist(samples, bins=50)\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array([ 0.61143659, -0.99900387,  0.48252666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " [-0.91292151,  3.09695411,  1.06725919,  2.02969492,  1.9801807 , 2.27330438],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[0][1][1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(G_sample, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array([[ 0.91292151],\n",
    "       [ 3.09695411],\n",
    "       [ 1.06725919],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sess.run(G_sample, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for it in range(20001):\n",
    "    if it % 100 == 0:\n",
    "        print(\"Training epoch %d\" %it)\n",
    "    batch_sched = list(range(0, n_train+1,S))\n",
    "    for j in range(0, len(batch_sched)-1):\n",
    "        start = batch_sched[j]\n",
    "        stop = batch_sched[j+1]\n",
    "        feed_dict = construct_feed_dict(train_dataset, start, stop)\n",
    "\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict=feed_dict)\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict=feed_dict)\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        samples = sess.run(G_sample, feed_dict=feed_dict)\n",
    "        plt.hist(samples, bins=50)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for k in range(0,100):\n",
    "    feed_dict = construct_feed_dict(train_dataset, 0, 1)\n",
    "    samples = sess.run(G_sample, feed_dict=feed_dict)[2]\n",
    "    preds.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(np.concatenate(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset[0][1][1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = gan.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(res, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[0][1][1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[0][1][1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(0,100):\n",
    "    preds.append(gan.predict(features)[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[0][2].atom_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
